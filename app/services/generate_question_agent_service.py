"""
ServiÃ§o de GeraÃ§Ã£o de QuestÃµes Educacionais.

Utiliza LLM (DeepSeek/OpenAI/Gemini) para gerar questÃµes de mÃºltipla escolha
baseadas em habilidades e nÃ­veis de proficiÃªncia especÃ­ficos.
"""

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence
from langchain_core.output_parsers import JsonOutputParser
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
import logging
import json
import re

from app.schemas.question_schema import QuestionListSchema
from app.schemas.request_body_agent import RequestBodyAgentQuestion
from app.enums.agente_prompt_template import AgentPromptTemplates, get_prompt
from app.core.llm_config import (
    get_question_llm,
    get_runnable_config,
    QuestionGenerationError,
    RETRY_CONFIG
)
from app.services.text_search_service import TextSearchService, TextSearchError

# Logger para este mÃ³dulo
logger = logging.getLogger(__name__)


class GenerateQuestionAgentService:
    """
    ServiÃ§o para geraÃ§Ã£o de questÃµes educacionais usando IA.
    
    Utiliza o modelo Gemini para gerar questÃµes
    estruturadas com base em parÃ¢metros de habilidade e proficiÃªncia.
    
    Attributes:
        llm: InstÃ¢ncia do modelo de linguagem configurado
        _chain_cache: Cache de chains criadas para cada template
        text_search_service: ServiÃ§o para busca de textos reais
    """
    
    def __init__(self):
        """Inicializa o serviÃ§o com configuraÃ§Ã£o centralizada do LLM."""
        self.llm = get_question_llm()
        self._chain_cache: dict[str, RunnableSequence] = {}
        
        logger.info("GenerateQuestionAgentService inicializado")
    
    def _parse_json_response(self, response_text: str) -> dict:
        """
        Faz parsing manual de JSON da resposta do LLM.
        
        Args:
            response_text: Texto da resposta do LLM
            
        Returns:
            DicionÃ¡rio com dados parseados
        """
        # Remove possÃ­veis blocos de cÃ³digo markdown
        text = response_text.strip()
        
        # Remove ```json e ``` se presentes
        if text.startswith("```"):
            lines = text.split("\n")
            # Remove primeira linha (```json ou similar)
            lines = lines[1:]
            # Remove Ãºltima linha se for ```
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            text = "\n".join(lines)
        
        # Encontra o inÃ­cio do JSON
        start_idx = text.find('{')
        if start_idx == -1:
            raise ValueError("Nenhum objeto JSON encontrado na resposta")
        
        # Extrai apenas o primeiro objeto JSON completo usando contador de chaves
        brace_count = 0
        end_idx = start_idx
        in_string = False
        escape_next = False
        
        for i, char in enumerate(text[start_idx:], start=start_idx):
            if escape_next:
                escape_next = False
                continue
            if char == '\\':
                escape_next = True
                continue
            if char == '"' and not escape_next:
                in_string = not in_string
                continue
            if in_string:
                continue
            if char == '{':
                brace_count += 1
            elif char == '}':
                brace_count -= 1
                if brace_count == 0:
                    end_idx = i + 1
                    break
        
        json_str = text[start_idx:end_idx]
        return json.loads(json_str)
    
    def _get_or_create_chain(
        self, 
        template: str,
        input_variables: list[str] = None
    ) -> RunnableSequence:
        """
        ObtÃ©m ou cria uma chain para o template especificado.
        
        Utiliza cache para evitar recriar chains para o mesmo template.
        
        Args:
            template: Template do prompt
            input_variables: Lista de variÃ¡veis do template
            
        Returns:
            Chain configurada (retorna texto bruto para parsing manual)
        """
        template_hash = hash(template)
        
        if template_hash not in self._chain_cache:
            # VariÃ¡veis padrÃ£o
            default_vars = [
                "count_questions",
                "count_alternatives", 
                "skill",
                "proficiency_level",
                "grade",
                "model_evaluation_type",
                "image_dependency_instruction"
            ]
            
            prompt = PromptTemplate(
                input_variables=input_variables or default_vars,
                template=template
            )
            
            # Usa LLM direto sem structured output para compatibilidade com DeepSeek
            self._chain_cache[template_hash] = prompt | self.llm
            logger.debug(f"Chain criada e cacheada (hash: {template_hash})")
        
        return self._chain_cache[template_hash]
    
    @retry(
        stop=stop_after_attempt(RETRY_CONFIG["stop_after_attempt"]),
        wait=wait_exponential(
            multiplier=RETRY_CONFIG["wait_exponential_multiplier"],
            min=RETRY_CONFIG["wait_exponential_min"],
            max=RETRY_CONFIG["wait_exponential_max"]
        ),
        retry=retry_if_exception_type((ConnectionError, TimeoutError)),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        reraise=True
    )
    def _invoke_chain(
        self,
        chain: RunnableSequence,
        inputs: dict,
        run_name: str
    ) -> QuestionListSchema:
        """
        Invoca a chain com retry automÃ¡tico.
        
        Args:
            chain: Chain a ser executada
            inputs: DicionÃ¡rio de inputs para o prompt
            run_name: Nome da execuÃ§Ã£o para rastreamento
            
        Returns:
            Schema de questÃµes geradas
            
        Raises:
            QuestionGenerationError: Se falhar apÃ³s todas as tentativas
        """
        config = get_runnable_config(
            run_name=run_name,
            tags=["question-generation"]
        )
        
        # Invoca e obtÃ©m resposta bruta
        response = chain.invoke(inputs, config=config)
        
        # Extrai conteÃºdo do AIMessage
        if hasattr(response, 'content'):
            response_text = response.content
        else:
            response_text = str(response)
        
        # Parse manual do JSON
        try:
            parsed_data = self._parse_json_response(response_text)
            return QuestionListSchema(**parsed_data)
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"Erro ao parsear resposta JSON: {e}")
            logger.debug(f"Resposta recebida: {response_text[:500]}...")
            raise QuestionGenerationError(f"Resposta do LLM nÃ£o Ã© JSON vÃ¡lido: {e}") from e

    def send_to_llm(
        self,
        template: str,
        query: RequestBodyAgentQuestion,
        extra_inputs: dict = None
    ) -> QuestionListSchema:
        """
        Envia requisiÃ§Ã£o para o LLM gerar questÃµes.
        
        Args:
            template: Template do prompt a ser usado
            query: ParÃ¢metros da requisiÃ§Ã£o (quantidade, habilidade, etc.)
            extra_inputs: Inputs adicionais para o template (ex: texto real)
            
        Returns:
            Lista de questÃµes geradas estruturadas
            
        Raises:
            QuestionGenerationError: Se ocorrer erro na geraÃ§Ã£o
        """
        logger.info(
            f"Gerando {query.count_questions} questÃµes - "
            f"Habilidade: {query.skill[:30]}... | "
            f"NÃ­vel: {query.proficiency_level} | "
            f"Modelo: {query.model_evaluation_type.value}"
        )
        
        # Determina as variÃ¡veis do template
        input_variables = [
            "count_questions",
            "count_alternatives", 
            "skill",
            "proficiency_level",
            "grade",
            "model_evaluation_type"
        ]
        
        # Adiciona variÃ¡veis extras se houver
        if extra_inputs:
            input_variables.extend(extra_inputs.keys())
        
        chain = self._get_or_create_chain(template, input_variables)
        
        # Mapeia a dependÃªncia de imagem para instruÃ§Ãµes
        image_instructions = {
            "none": """âš ï¸ QUESTÃƒO SEM IMAGEM - REGRAS ABSOLUTAS âš ï¸

âŒ PROIBIDO GERAR QUESTÃ•ES QUE DEPENDAM DE IMAGEM:
   - NÃƒO use "[IMAGEM: ...]" no texto
   - NÃƒO mencione "observe a figura", "observe a imagem", "observe a tirinha"
   - NÃƒO faÃ§a questÃµes sobre grÃ¡ficos, tabelas visuais, charges ou tirinhas
   - NÃƒO referencie elementos visuais que o aluno precisaria ver

âœ… OBRIGATÃ“RIO:
   - As questÃµes devem ser 100% resolvidas apenas com LEITURA DO TEXTO
   - Todo o conteÃºdo necessÃ¡rio deve estar ESCRITO no texto-base
   - Se precisar de dados numÃ©ricos, ESCREVA-OS no texto (nÃ£o em grÃ¡ficos)
   - O campo "text" deve conter o texto completo necessÃ¡rio para resolver a questÃ£o

ðŸŽ¯ PRINCÃPIO: O aluno resolve a questÃ£o APENAS LENDO, sem precisar de nenhuma imagem.""",
            "optional": "As questÃµes podem opcionalmente ter imagens ilustrativas decorativas, mas a resoluÃ§Ã£o NÃƒO deve depender da imagem. O aluno deve conseguir responder apenas com o texto.",
            "required": """âš ï¸âš ï¸âš ï¸ QUESTÃƒO OBRIGATORIAMENTE DEPENDENTE DE IMAGEM âš ï¸âš ï¸âš ï¸

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸŽ¯ REGRA PRINCIPAL: A questÃ£o SÃ“ PODE ser resolvida OLHANDO para a imagem.
   Se o aluno conseguir responder APENAS lendo o texto, a questÃ£o estÃ¡ ERRADA!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… ESTRUTURA OBRIGATÃ“RIA:
   1. O campo "text" deve ser CURTO: "Observe a imagem/grÃ¡fico/figura a seguir."
      - NÃƒO coloque dados, tabelas ou descriÃ§Ãµes no campo "text"
      - Os dados essenciais estarÃ£o NA IMAGEM (que serÃ¡ gerada depois)
   2. O "question_statement" deve EXIGIR anÃ¡lise visual:
      - "De acordo com o grÃ¡fico, qual foi..."
      - "Observando a figura, qual Ã© a medida de..."
      - "Com base na imagem, Ã© possÃ­vel concluir que..."
      - "A partir dos dados apresentados no grÃ¡fico..."
   3. As alternativas devem requerer INTERPRETAÃ‡ÃƒO da imagem + raciocÃ­nio

ðŸŽ¯ TIPOS DE QUESTÃƒO COM IMAGEM (escolha um):
   ðŸ“Š GRÃFICOS: barras, pizza, linha â€” o aluno precisa LER valores do grÃ¡fico
   ðŸ“ FIGURAS GEOMÃ‰TRICAS: triÃ¢ngulos, retÃ¢ngulos â€” o aluno precisa EXTRAIR medidas da figura
   ðŸ—ºï¸ MAPAS/DIAGRAMAS: o aluno precisa INTERPRETAR o diagrama visual
   ðŸ“ˆ TABELAS VISUAIS: dados organizados que sÃ³ existem na imagem
   ðŸ–¼ï¸ CENAS/TIRINHAS: o aluno precisa OBSERVAR elementos visuais

ðŸš«ðŸš«ðŸš« REGRA DE OURO: JAMAIS DESCREVA A IMAGEM ðŸš«ðŸš«ðŸš«
   A imagem serÃ¡ gerada SEPARADAMENTE por outra IA.
   VocÃª NÃƒO SABE como a imagem serÃ¡. NÃƒO INVENTE descriÃ§Ãµes.

   âŒ ERRADO (descreve a imagem no texto ou enunciado):
      - "O grÃ¡fico mostra que 40% dos alunos preferem futebol"
      - "Na imagem, hÃ¡ um triÃ¢ngulo retÃ¢ngulo com catetos de 3cm e 4cm"
      - "A tirinha apresenta um personagem surpreso"
      - "O mapa indica a regiÃ£o Nordeste do Brasil"
      - "A tabela mostra os dados de vendas de janeiro a marÃ§o"
      - "Observe o grÃ¡fico de barras que apresenta a quantidade de livros lidos"
      - "[IMAGEM: grÃ¡fico de barras com dados de...]"

   âœ… CORRETO (apenas referencia sem descrever):
      - "Observe o grÃ¡fico a seguir." (SEM dizer o que o grÃ¡fico mostra)
      - "Observe a figura a seguir." (SEM descrever a figura)
      - "Analise a imagem e responda." (SEM descrever o conteÃºdo)

ðŸš« TAMBÃ‰M PROIBIDO:
   - NÃƒO coloque os dados do problema no campo "text" (eles devem estar NA IMAGEM)
   - NÃƒO escreva tabelas/grÃ¡ficos em formato texto â€” eles serÃ£o visuais
   - NÃƒO crie questÃµes que possam ser respondidas sem ver a imagem
   - NÃƒO use colchetes [IMAGEM: ...] ou [FIGURA: ...]

ðŸ“ COERÃŠNCIA MATEMÃTICA:
   - Valores numÃ©ricos devem ser matematicamente consistentes
   - A RESPOSTA deve exigir CÃLCULO ou INFERÃŠNCIA, nÃ£o observaÃ§Ã£o direta
   - HIPOTENUSA Ã© SEMPRE o maior lado do triÃ¢ngulo retÃ¢ngulo

ðŸŽ¯ TESTE FINAL: Leia sua questÃ£o SEM a imagem. Se conseguir responder, REFAÃ‡A!
   O aluno DEVE OLHAR a imagem + RACIOCINAR para responder."""
        }
        
        inputs = {
            "count_questions": query.count_questions,
            "count_alternatives": query.count_alternatives,
            "skill": query.skill,
            "proficiency_level": query.proficiency_level,
            "grade": query.grade,
            "model_evaluation_type": query.model_evaluation_type.value,
            "image_dependency_instruction": image_instructions.get(query.image_dependency, image_instructions["none"])
        }
        
        # Mescla inputs extras
        if extra_inputs:
            inputs.update(extra_inputs)
        
        try:
            response = self._invoke_chain(
                chain=chain,
                inputs=inputs,
                run_name=f"question-gen-{query.grade}-{query.count_questions}q"
            )
            
            logger.info(
                f"âœ… GeraÃ§Ã£o concluÃ­da: {len(response.questions)} questÃµes"
            )
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erro ao gerar questÃµes: {e}")
            raise QuestionGenerationError(
                f"Falha ao gerar questÃµes apÃ³s mÃºltiplas tentativas: {e}"
            ) from e
    
    def generate_with_real_text(
        self,
        query: RequestBodyAgentQuestion
    ) -> QuestionListSchema:
        """
        Gera questÃµes usando conhecimento interno do LLM para recuperar textos reais.
        
        Args:
            query: ParÃ¢metros da requisiÃ§Ã£o
            
        Returns:
            Lista de questÃµes geradas com textos autÃªnticos recuperados da memÃ³ria do LLM
        """
        logger.info("ðŸ” Modo: RecuperaÃ§Ã£o de Textos Reais via Conhecimento da IA (Sem busca externa)")
        
        # Usa DIRETAMENTE o template de retrieval da IA
        # A IA serÃ¡ responsÃ¡vel por buscar em sua base de treinamento textos que atendam Ã  habilidade
        template = get_prompt(AgentPromptTemplates.AI_RETRIEVAL_PT_TEMPLATE)
        
        # Adicionamos instruÃ§Ã£o extra para garantir variabilidade
        extra_inputs = {
            "variability_instruction": "IMPORTANTE: Gere questÃµes TOTALMENTE DISTINTAS. Se houver mais de uma questÃ£o, use TEXTOS DIFERENTES para cada uma. DIVERSIFIQUE OS AUTORES DA LITERATURA BRASILEIRA."
        }
        
        return self.send_to_llm(template, query, extra_inputs)
    
    def generate_questions(
        self,
        query: RequestBodyAgentQuestion
    ) -> QuestionListSchema:
        """
        Gera questÃµes usando pipeline multi-agente LangGraph com garantia de qualidade.
        
        O fluxo LangGraph Ã©:
        1. Router â†’ decide se busca textos reais
        2. Searcher â†’ busca textos (se use_real_text=True)
        3. Agente Gerador â†’ cria questÃµes padrÃ£o SAEB
        4. Agente Revisor â†’ avalia qualidade BNCC/SAEB
        5. Router de Qualidade â†’ regenera se score < 0.7, mÃ¡x 3 tentativas
        
        Args:
            query: ParÃ¢metros da requisiÃ§Ã£o
            
        Returns:
            Lista de questÃµes geradas e validadas
            
        Raises:
            QuestionGenerationError: Se ocorrer erro na geraÃ§Ã£o
        """
        from app.services.langgraph_orchestrator import get_orchestrator
        
        logger.info("ðŸš€ Usando LangGraph Multi-Agent para geraÃ§Ã£o (padrÃ£o SAEB)")
        
        try:
            orchestrator = get_orchestrator()
            return orchestrator.generate(query)
        except Exception as e:
            logger.error(f"âŒ Erro na geraÃ§Ã£o LangGraph: {e}")
            raise QuestionGenerationError(f"Falha na geraÃ§Ã£o de questÃµes: {e}") from e