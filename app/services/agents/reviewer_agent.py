"""
Agente Revisor de QuestÃµes.

ResponsÃ¡vel por avaliar a qualidade pedagÃ³gica das questÃµes geradas,
verificando alinhamento BNCC, distratores, clareza e proficiÃªncia.
"""

import logging
import json
from typing import List, Dict, Any

from langchain_core.prompts import PromptTemplate

from app.services.agents.state import AgentState
from app.core.llm_config import get_question_llm, get_runnable_config
from app.services.progress_manager import get_current_progress

logger = logging.getLogger(__name__)


REVIEWER_PROMPT = """
VocÃª Ã© um especialista em avaliaÃ§Ãµes educacionais brasileiras (SAEB, SEAMA, BNCC).

Sua tarefa Ã© revisar as questÃµes geradas e avaliar a qualidade pedagÃ³gica.

QUESTÃ•ES PARA REVISAR:
{questions_json}

HABILIDADE SOLICITADA: {skill}
NÃVEL DE PROFICIÃŠNCIA: {proficiency_level}
ANO/SÃ‰RIE: {grade}

---

Para CADA questÃ£o, avalie os seguintes CRITÃ‰RIOS (nota de 0 a 10):

1. **ALINHAMENTO_BNCC**: A questÃ£o contempla corretamente a habilidade?
2. **DISTRATORES**: Avalie rigorosamente a qualidade dos distratores:
   a) Cada alternativa incorreta Ã© PLAUSÃVEL (nÃ£o Ã³bvia/absurda)?
   b) Cada distrator representa um ERRO CONCEITUAL REAL (leitura superficial, extrapolaÃ§Ã£o, reduÃ§Ã£o, contradiÃ§Ã£o, foco irrelevante, erro de cÃ¡lculo, confusÃ£o de conceitos)?
   c) Os distratores tÃªm MESMA ESTRUTURA GRAMATICAL e TAMANHO SIMILAR (Â±20%) ao gabarito?
   d) O campo "distractor" de cada alternativa EXPLICA o erro conceitual especÃ­fico?
   e) NENHUM distrator Ã© absurdo, humorÃ­stico, fora de contexto ou gramaticalmente inconsistente?
   Nota 10: todos os critÃ©rios. Nota 5: plausÃ­veis mas sem tipagem de erro. Nota 0: absurdos/Ã³bvios.
3. **CLAREZA**: O enunciado Ã© claro, sem ambiguidades ou erros?
4. **PROFICIENCIA**: O nÃ­vel de dificuldade estÃ¡ adequado ao nÃ­vel solicitado?
5. **TEXTO_BASE**: O texto suporte Ã© relevante, autÃªntico e apropriado?
6. **COERENCIA_IMAGEM**: Se a questÃ£o usa imagem, verifique:
   - A resposta NÃƒO Ã© diretamente visÃ­vel na imagem (requer cÃ¡lculo/inferÃªncia)?
   - Se for matemÃ¡tica: hipotenusa Ã© o maior lado? ProporÃ§Ãµes sÃ£o corretas?
   - O enunciado NÃƒO repete dados que jÃ¡ aparecem na imagem?
   - O texto NÃƒO descreve detalhadamente a imagem?
   (Se nÃ£o usa imagem, dÃª nota 10)
7. **COERENCIA_MATEMATICA_3D**: Se a questÃ£o envolve geometria espacial (pirÃ¢mide, cone, prisma), verifique:
   - A TERMINOLOGIA estÃ¡ correta? (aresta lateral â‰  apÃ³tema da pirÃ¢mide)
   - Aresta lateral de pirÃ¢mide = Vâ†’vÃ©rtice da base, usa metade da DIAGONAL ((ladoÃ—âˆš2)/2)
   - ApÃ³tema da pirÃ¢mide = Vâ†’ponto mÃ©dio do lado, usa apÃ³tema da BASE (lado/2)
   - O CÃLCULO numÃ©rico usa a fÃ³rmula correspondente ao TERMO do enunciado?
   - A resposta numÃ©rica bate com a fÃ³rmula correta?
   - O "?" na imagem marca o segmento correto?
   (Se nÃ£o envolve geometria 3D, dÃª nota 10)

---

RESPONDA EXCLUSIVAMENTE no formato JSON abaixo:
{{
    "reviews": [
        {{
            "question_number": 1,
            "scores": {{
                "alinhamento_bncc": X,
                "distratores": X,
                "clareza": X,
                "proficiencia": X,
                "texto_base": X,
                "coerencia_imagem": X,
                "coerencia_matematica_3d": X
            }},
            "issues": ["Lista de problemas encontrados, se houver"],
            "suggestions": ["SugestÃµes de melhoria, se necessÃ¡rio"]
        }}
    ],
    "overall_score": X.X,
    "approved": true/false,
    "summary_feedback": "Resumo geral do feedback para regeneraÃ§Ã£o, se reprovado"
}}

REGRAS:
- overall_score = mÃ©dia de todas as notas / 10 (resultado entre 0.0 e 1.0)
- approved = true se overall_score >= 0.7
- Se approved = false, preencha summary_feedback com instruÃ§Ãµes claras de correÃ§Ã£o
- ESPECIALMENTE verifique se questÃµes com imagem tÃªm a RESPOSTA visÃ­vel (isso Ã© GRAVE)
"""


def _parse_review_response(response_text: str) -> dict:
    """Parse da resposta JSON do revisor."""
    text = response_text.strip()
    
    if text.startswith("```"):
        lines = text.split("\n")
        lines = lines[1:]
        if lines and lines[-1].strip() == "```":
            lines = lines[:-1]
        text = "\n".join(lines)
    
    start_idx = text.find('{')
    if start_idx == -1:
        raise ValueError("JSON nÃ£o encontrado na resposta do revisor")
    
    brace_count = 0
    end_idx = start_idx
    in_string = False
    escape_next = False
    
    for i, char in enumerate(text[start_idx:], start=start_idx):
        if escape_next:
            escape_next = False
            continue
        if char == '\\':
            escape_next = True
            continue
        if char == '"' and not escape_next:
            in_string = not in_string
            continue
        if in_string:
            continue
        if char == '{':
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0:
                end_idx = i + 1
                break
    
    json_str = text[start_idx:end_idx]
    return json.loads(json_str)


def reviewer_node(state: AgentState) -> AgentState:
    """
    NÃ³ do Agente Revisor.
    
    Avalia a qualidade pedagÃ³gica das questÃµes e retorna
    uma pontuaÃ§Ã£o de qualidade e feedback para regeneraÃ§Ã£o.
    
    Args:
        state: Estado atual do grafo
        
    Returns:
        Estado atualizado com pontuaÃ§Ã£o e feedback
    """
    questions = state.get("questions", [])
    query = state["query"]
    
    if not questions:
        logger.warning("âš ï¸ Revisor: Nenhuma questÃ£o para revisar")
        return {
            **state,
            "quality_score": 0.0,
            "revision_feedback": "Nenhuma questÃ£o foi gerada. Tente novamente."
        }
    
    logger.info(f"ğŸŸ¡ Agente Revisor - Avaliando {len(questions)} questÃµes")
    
    progress = get_current_progress()
    
    try:
        if progress:
            progress.log("reviewer", "Initializing review LLM", "", "ğŸ”Œ")
            progress.log("reviewer", "Loading 7 quality criteria (BNCC, Distractors, Clarity...)", "", "ğŸ“‹")
        llm = get_question_llm()
        
        # Prepara o prompt
        prompt = PromptTemplate(
            input_variables=["questions_json", "skill", "proficiency_level", "grade"],
            template=REVIEWER_PROMPT
        )
        
        chain = prompt | llm
        config = get_runnable_config(
            run_name="reviewer-evaluation",
            tags=["langgraph", "reviewer"]
        )
        
        inputs = {
            "questions_json": json.dumps(questions, ensure_ascii=False, indent=2),
            "skill": query.skill,
            "proficiency_level": query.proficiency_level,
            "grade": query.grade
        }
        
        if progress:
            progress.log("reviewer", "Building evaluation prompt", f"{len(questions)} questions to review", "ğŸ“‹")
            progress.log("reviewer", "Checking distractor plausibility (5 sub-criteria)", "", "ğŸ­")
            progress.log("reviewer", "Calling DeepSeek API (review)...", "", "ğŸš€")
        response = chain.invoke(inputs, config=config)
        response_text = response.content if hasattr(response, 'content') else str(response)
        if progress:
            progress.log("reviewer", "Review response received", "", "ğŸ“¥")
            progress.log("reviewer", "Analyzing scores per criterion...", "", "ğŸ“Š")
        
        # Parse da revisÃ£o
        if progress:
            progress.log("reviewer", "Parsing evaluation response", "", "ğŸ”§")
        review_data = _parse_review_response(response_text)
        
        overall_score = review_data.get("overall_score", 0.0)
        approved = review_data.get("approved", False)
        feedback = review_data.get("summary_feedback", None) if not approved else None
        
        logger.info(
            f"{'âœ…' if approved else 'âš ï¸'} Revisor - Score: {overall_score:.2f} | "
            f"Aprovado: {approved}"
        )
        
        if progress:
            reviews = review_data.get("reviews", [])
            for rev in reviews:
                qnum = rev.get("question_number", "?")
                scores = rev.get("scores", {})
                for criteria, score_val in scores.items():
                    criteria_label = criteria.replace("_", " ").title()
                    progress.log("reviewer", f"Q{qnum} â€” {criteria_label}: {score_val}/10", "", "ğŸ“")
                issues = rev.get("issues", [])
                for issue in issues:
                    progress.log("reviewer", f"Q{qnum} issue: {issue[:80]}", "", "âš ï¸")
            
            score_pct = f"{overall_score * 100:.0f}%"
            progress.metric("reviewer", "Overall quality score", score_pct, "ğŸ¯")
            progress.metric("reviewer", "Approved", "âœ… Yes" if approved else "âŒ No", "ğŸ“‹")
        
        if not approved and feedback:
            logger.info(f"ğŸ“ Feedback: {feedback[:100]}...")
            if progress:
                progress.log("reviewer", f"Feedback: {feedback[:120]}", "", "ğŸ“")
        
        return {
            **state,
            "quality_score": overall_score,
            "revision_feedback": feedback
        }
        
    except Exception as e:
        logger.error(f"âŒ Erro no Agente Revisor: {e}")
        if progress:
            progress.log("reviewer", f"Error: {str(e)[:120]}", "", "âŒ")
        # Em caso de erro, aprova para nÃ£o travar o fluxo
        return {
            **state,
            "quality_score": 0.75,  # Score neutro para continuar
            "revision_feedback": None,
            "error": f"Erro na revisÃ£o: {e}"
        }
